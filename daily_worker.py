import pandas as pd
import gspread
import re
import os
import json
import base64
import requests
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client
from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request

# --- C·∫§U H√åNH ---
SPREADSHEET_ID = '15Q7_YzBYMjCceBB5-yi51noA0d03oqRIcd-icDvCdqI'

# L·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (Github) ho·∫∑c hardcode (Local)
SUPABASE_URL = os.environ.get("SUPABASE_URL", "https://wpzigasfuizrabqqzxln.supabase.co")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "sb_secret_tPw7wEcEku1sVGVITE2X7A_MNtKlCww")
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "AIzaSyChr_rRRYlsH9_wfY8JB1UJ30fPDMBtp0c") # Thay key c·ªßa m√†y v√†o

# --- M√öI GI·ªú H√Ä N·ªòI (GMT+7) ---
def get_hanoi_time():
    # T·∫°o timezone GMT+7
    tz_vn = timezone(timedelta(hours=7))
    return datetime.now(tz_vn)

# --- AUTHENTICATION ---
def get_gspread_client():
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
    
    # 1. ∆Øu ti√™n l·∫•y t·ª´ Github Secret (Base64)
    if os.environ.get("TOKEN_JSON_BASE64"):
        print("üîë ƒêang d√πng Token t·ª´ Github Secret...")
        try:
            token_json_str = base64.b64decode(os.environ.get("TOKEN_JSON_BASE64")).decode('utf-8')
            token_info = json.loads(token_json_str)
            creds = Credentials.from_authorized_user_info(token_info, SCOPES)
        except Exception as e:
            raise Exception(f"‚ùå L·ªói decode token base64: {e}")
            
    # 2. N·∫øu kh√¥ng c√≥ th√¨ t√¨m file local
    elif os.path.exists('token.json'):
        print("üîë ƒêang d√πng Token t·ª´ file Local...")
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    else:
        raise Exception("‚ùå Kh√¥ng t√¨m th·∫•y Token ƒëƒÉng nh·∫≠p (token.json ho·∫∑c ENV Var)!")

    # Auto refresh token n·∫øu h·∫øt h·∫°n
    if creds.expired and creds.refresh_token:
        creds.refresh(Request())

    return gspread.authorize(creds)

# Init Clients
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
gc = get_gspread_client()

# --- HELPERS ---
def extract_video_id(url):
    if not isinstance(url, str): return None
    match = re.search(r'(?:v=|\/)([\w-]{11})(?=&|\?|$)', url)
    return match.group(1) if match else None

# --- TASK 1: SYNC T·ª™ SHEET PROGRESS -> SUPABASE ---
def sync_progress_to_db():
    print("\n>>> TASK 1: Syncing Metadata (Progress -> DB)...")
    try:
        sh = gc.open_by_key(SPREADSHEET_ID)
        ws = sh.worksheet('KOL PROGRESS')
        records = ws.get_all_records()
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc sheet Progress: {e}")
        return

    count_new = 0

    for row in records:
        kol_name = str(row.get('Name', '')).strip()
        report_links = str(row.get('Report Link', '')).split('\n')
        
        email = row.get('Email', '')
        country = row.get('Location', '')
        sub_count = str(row.get('Subscriber/Follower', ''))
        agreement = row.get('Signed Agreement', '')
        package = str(row.get('Total Package', ''))
        
        # --- NEW UPDATE: L·∫•y content count ---
        try:
            raw_count = row.get('No. Of Content', 0)
            content_count = int(str(raw_count).replace(',', '').strip()) if raw_count else 0
        except:
            content_count = 0
        # -------------------------------------

        if not kol_name: continue

        # 1. Upsert KOL
        kol_data = {
            'name': kol_name,
            'email': email,
            'country': country,
            'subscriber_count': sub_count
        }
        kol_res = supabase.table('kols').upsert(kol_data, on_conflict='name').execute()
        
        if kol_res.data: kol_id = kol_res.data[0]['id']
        else: 
            try:
                kol_id = supabase.table('kols').select('id').eq('name', kol_name).execute().data[0]['id']
            except: continue

        # 2. Upsert Videos
        for link in report_links:
            clean_link = link.strip()
            if not clean_link: continue
            
            video_data = {
                'kol_id': kol_id,
                'video_url': clean_link,
                'agreement_link': agreement,
                'total_package': package,
                'content_count': content_count, # <--- D√≤ng m·ªõi th√™m
                'status': 'Active'
            }
            try:
                supabase.table('videos').upsert(video_data, on_conflict='video_url').execute()
                count_new += 1
            except Exception as e:
                pass 

    print(f"‚úÖ ƒê√£ ƒë·ªìng b·ªô metadata (t√¨m th·∫•y {count_new} link ti·ªÅm nƒÉng).")
# --- TASK 2: TRACK VIEW (YOUTUBE API -> DB) ---
def track_youtube_views():
    print("\n>>> TASK 2: Tracking Youtube Views...")
    
    # L·∫•y list video Active
    try:
        videos = supabase.table('videos').select('*').eq('status', 'Active').execute().data
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc Supabase: {e}")
        return
    
    valid_videos = []
    for v in videos:
        vid = extract_video_id(v['video_url'])
        if vid:
            v['yt_id'] = vid
            valid_videos.append(v)
    
    print(f"üîç Scan {len(valid_videos)} videos...")

    chunk_size = 50
    # FIX: L·∫•y ng√†y theo gi·ªù H√† N·ªôi (GMT+7)
    now_vn = get_hanoi_time()
    today_str = now_vn.strftime('%Y-%m-%d') 
    
    updated_count = 0

    for i in range(0, len(valid_videos), chunk_size):
        chunk = valid_videos[i:i+chunk_size]
        ids = ",".join([v['yt_id'] for v in chunk])
        
        try:
            url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id={ids}&key={YOUTUBE_API_KEY}"
            res = requests.get(url).json()
            
            metrics_insert = []
            
            for item in res.get('items', []):
                yt_id = item['id']
                stats = item['statistics']
                snippet = item['snippet']
                
                view_count = int(stats.get('viewCount', 0))
                title = snippet.get('title', '')
                published_at = snippet.get('publishedAt', '').split('T')[0]

                # Map v·ªõi DB
                db_vid = next((v for v in chunk if v['yt_id'] == yt_id), None)
                if db_vid:
                    # 1. Chu·∫©n b·ªã data Metrics (L·ªãch s·ª≠ h√¥m nay)
                    metrics_insert.append({
                        'video_id': db_vid['id'],
                        'view_count': view_count,
                        'recorded_at': today_str 
                        # created_at s·∫Ω t·ª± ƒë·ªông l·∫•y gi·ªù server (UTC), ko c·∫ßn ch·ªânh
                    })

                    # 2. T√≠nh Growth (So v·ªõi 7 ng√†y tr∆∞·ªõc)
                    # Logic: L·∫•y ng√†y hi·ªán t·∫°i - 7 ng√†y
                    date_7_ago = (now_vn - timedelta(days=7)).strftime('%Y-%m-%d')
                    
                    hist = supabase.table('video_metrics').select('view_count')\
                        .eq('video_id', db_vid['id'])\
                        .eq('recorded_at', date_7_ago)\
                        .execute()
                    
                    # N·∫øu t√¨m th·∫•y view c≈© th√¨ tr·ª´, ko th√¨ coi growth = 0 (ho·∫∑c b·∫±ng view hi·ªán t·∫°i n·∫øu l√† video m·ªõi tinh)
                    view_7_days_ago = hist.data[0]['view_count'] if hist.data else view_count
                    growth = view_count - view_7_days_ago

                    # 3. Update Cache & Metadata v√†o b·∫£ng Videos
                    # Logic title: N·∫øu API tr·∫£ v·ªÅ r·ªóng, gi·ªØ nguy√™n c≈©, ho·∫∑c d√πng URL
                    final_title = title if title else (db_vid.get('title') or db_vid.get('video_url'))

                    supabase.table('videos').update({
                        'title': final_title,
                        'released_date': published_at,
                        'current_views': view_count,
                        'last_7_days_views': growth
                    }).eq('id', db_vid['id']).execute()
            
            if metrics_insert:
                # FIX: Upsert d·ª±a tr√™n (video_id, recorded_at)
                # ƒê·∫£m b·∫£o 1 ng√†y ch·ªâ c√≥ 1 d√≤ng, ch·∫°y l·∫°i s·∫Ω update view
                supabase.table('video_metrics').upsert(metrics_insert, on_conflict='video_id,recorded_at').execute()
                updated_count += len(metrics_insert)

        except Exception as e:
            print(f"‚ùå L·ªói batch Youtube API: {e}")

    print(f"‚úÖ ƒê√£ update view cho {updated_count} videos (Ng√†y recorded: {today_str}).")

# --- TASK 3: BUILD DASHBOARD (DB -> SHEET FRONTEND) ---
def build_dashboard():
    print("\n>>> TASK 3: Building KOL DASHBOARD...")
    
    # Query Data
    try:
        res = supabase.table('videos').select('*, kols(name, country, subscriber_count)').order('released_date', desc=True).execute()
        data = res.data
    except Exception as e:
        print(f"‚ùå L·ªói query Supabase Dashboard: {e}")
        return

    headers = ['Video Title', 'KOL Name', 'Country', 'Released', 'Total Views', 'Growth (7 Days)', 'Agreement', 'Package', 'Status']
    rows = []
    
    for item in data:
        # FIX: X·ª≠ l√Ω Title r·ªóng -> L·∫•y URL
        raw_title = item.get('title')
        video_url = item.get('video_url', '')
        
        display_title = raw_title if raw_title and str(raw_title).strip() != "" else video_url
        display_title = str(display_title).replace('"', '""') # Escape cho c√¥ng th·ª©c

        # Hyperlink Formula
        title_cell = f'=HYPERLINK("{video_url}", "{display_title}")'

        agreement_link = item.get('agreement_link', '')
        agreement_cell = f'=HYPERLINK("{agreement_link}", "View Contract")' if agreement_link else "-"

        kol_info = item.get('kols', {}) or {}
        kol_name = kol_info.get('name', 'Unknown')
        country = kol_info.get('country', '')

        views = item.get('current_views', 0)
        growth = item.get('last_7_days_views', 0)
        
        # Icon Growth
        growth_display = f"{growth:,}" 
        if growth > 0: growth_display = "üü¢ +" + growth_display
        elif growth == 0: growth_display = "‚ö™ " + growth_display
        else: growth_display = "üî¥ " + growth_display

        row = [
            title_cell,
            kol_name,
            country,
            item.get('released_date'),
            views,
            growth_display,
            agreement_cell,
            item.get('total_package'),
            item.get('status')
        ]
        rows.append(row)

    # Ghi v√†o Sheet
    try:
        sh = gc.open_by_key(SPREADSHEET_ID)
        try:
            ws = sh.worksheet('KOL DASHBOARD')
            ws.clear()
        except:
            ws = sh.add_worksheet(title='KOL DASHBOARD', rows=1000, cols=20)

        ws.update(range_name='A1', values=[headers])
        ws.format('A1:I1', {'textFormat': {'bold': True}, 'horizontalAlignment': 'CENTER', 'backgroundColor': {'red': 0.8, 'green': 0.8, 'blue': 0.8}})

        if rows:
            ws.update(range_name='A2', values=rows, value_input_option='USER_ENTERED')
            ws.format(f'E2:E{len(rows)+1}', {'numberFormat': {'type': 'NUMBER', 'pattern': '#,##0'}})
            # Set filter
            ws.set_basic_filter(f'A1:I{len(rows)+1}') 
            
        print("‚úÖ Dashboard built successfully!")
    except Exception as e:
        print(f"‚ùå L·ªói ghi Google Sheet: {e}")

# --- MAIN ---
if __name__ == "__main__":
    try:
        sync_progress_to_db()
        track_youtube_views()
        build_dashboard()
        print("\nüöÄ ALL TASKS COMPLETED!")
    except Exception as e:
        print(f"\n‚ùå FATAL ERROR: {e}")